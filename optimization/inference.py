import argparse
import datetime
import itertools
import json
import os
from typing import Dict, List, Tuple, Union

import constraints
import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
from config import parse_config_dict
from datasets import FolktablesLoader, SimulationLoader
from tqdm import tqdm


def read_json(config_filename):
    with open(config_filename, "r") as config_file:
        config_dict = json.load(config_file)
    return config_dict


def get_feature_strata(
    df: pd.DataFrame, levels: List[List[str]], target: str
) -> Dict[Tuple, pd.DataFrame]:
    """Generate strata for each combination of levels. The strata are
    generated by filtering the dataframe by the combination of levels
    and the target value.

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe with full dataset.
    levels : List[List[str]]
        Levels for each feature we want to create strata for.
    target : str
        Name of the target column.

    Returns
    -------
    Dict[Tuple, pd.DataFrame]
        Dictionary with strata dataframes for each combination of levels.
    """
    levels_combinations = list(itertools.product(*levels))
    strata_dfs = dict()
    for combination in levels_combinations:
        query_string = " & ".join([f"`{level}` == 1" for level in combination])
        strata_dfs[(0,) + combination] = df[df[target] == 0].query(query_string)

    for combination in levels_combinations:
        query_string = " & ".join([f"`{level}` == 1" for level in combination])
        strata_dfs[(1,) + combination] = df[df[target] == 1].query(query_string)
    return strata_dfs


def get_strata_counts(
    strata_dfs: Dict[Tuple, pd.DataFrame], levels: List[List[str]]
) -> torch.Tensor:
    """Get strata counts for each combination of levels. If a strata
    dataframe is empty, we set the count to a small value to avoid
    division by zero.

    Parameters
    ----------
    strata_dfs : Dict[Tuple, pd.DataFrame]
        Dictionary with strata dataframes for each combination of levels.
    levels : List[List[str]]
        Levels for each feature we want to create strata for.

    Returns
    -------
    torch.Tensor
        Tensor with strata counts for each combination of levels.
        The first dimension is the target value (0 or 1) and the
        remaining dimensions are the levels.
    """
    strata_counts = list()
    for strata_df in strata_dfs.values():
        if strata_df.shape[0] == 0:
            strata_counts.append(0.00001)
        else:
            strata_counts.append(strata_df.shape[0])

    output_shape = [2] + [len(level) for level in levels]
    return torch.tensor(strata_counts).reshape(output_shape)


def get_strata_covs(
    strata_dfs: Dict[Tuple, pd.DataFrame],
    levels: List[List[str]],
    all_feature_means: Dict[str, float],
) -> List[torch.Tensor]:
    """Get strata covariances for each combination of levels. If a strata
    dataframe is empty, we set the covariance to zero. We calculate
    the covariance for each pair of feature provided in all_feature_means.

    Parameters
    ----------
    strata_dfs : Dict[Tuple, pd.DataFrame]
        Dictionary with strata dataframes for each combination of levels.
    levels : List[List[str]]
        Levels for each feature we want to create strata for.
    all_feature_means : Dict[str, float]
        Dictionary with two feature names as keys and its means as values.

    Returns
    -------
    List[torch.Tensor]
        List of tensors with strata covariances for each pair of features
        provided in all_feature_means.
    """
    all_strata_covs = []
    for feature_means in all_feature_means:
        feature_names = list(feature_means.keys())
        means = list(feature_means.values())
        strata_covs = list()
        for strata_df in strata_dfs.values():
            strata_cov = np.sum(
                (strata_df[feature_names[0]] - means[0])
                * (strata_df[feature_names[1]] - means[1])
            )
            n_samples = strata_df.shape[0]
            if n_samples > 0:
                strata_cov *= 1 / strata_df.shape[0]
            else:
                strata_cov = 0
            if np.isnan(strata_cov):
                strata_cov = 0
            strata_covs.append(strata_cov)

        output_shape = [2] + [len(level) for level in levels]
        all_strata_covs.append(torch.tensor(strata_covs).reshape(output_shape))
    return all_strata_covs


def compute_f_divergence(
    p: torch.Tensor, q: torch.Tensor, type: str = "chi2"
) -> torch.Tensor:
    """Computes the f-divergence between two distributions.
    We ignore terms where q is zero to avoid division by zero.

    Parameters
    ----------
    p : torch.Tensor
        Terms of the first distribution.
    q : torch.Tensor
        Terms of the second distribution.
    type : str, optional
        Type of divergence to calculate, by default "chi2"

    Returns
    -------
    torch.Tensor
        f-divergence between the two distributions.

    Raises
    ------
    ValueError
        Raises ValueError if the type of divergence is not supported.
    """
    if type == "chi2":
        numerator = (p - q) ** 2
        denominator = q + 1e-8
        # Only include terms where q is non-zero
        mask = denominator > 1e-7
        numerator = numerator[mask]
        denominator = denominator[mask]
        return 0.5 * torch.sum(numerator / denominator)
    else:
        raise ValueError(f"Invalid divergence type {type}.")


def get_optimized_rho(
    restrictions: constraints.Restrictions,
    strata_estimands: Dict[str, torch.Tensor],
    feature_weights: torch.Tensor,
    n_iters: int,
) -> Tuple[float, List[float]]:
    """Gets the rho value that maximizes the f-divergence between the
    population and sample distributions. This is used to calculate a
    realistic benchmark when using DRO.

    Parameters
    ----------
    restrictions : constraints.Restrictions
        Set of constraints to be used in the optimization.
    strata_estimands : Dict[str, torch.Tensor]
        Dictionary with strata counts for each combination of levels.
    feature_weights : torch.Tensor
        Tensor that parametrizes the restrictions matrix
    n_iters : int
        Number of iterations to run the optimization for rho.

    Returns
    -------
    Tuple[float, List[float]]
        Tuple with the optimized rho value and the history of rho values.
    """
    data_count_0 = strata_estimands["count"][0]
    data_count_1 = strata_estimands["count"][1]
    dataset_size = restrictions.dataset.population_df_colinear.shape[0]

    torch.autograd.set_detect_anomaly(True)
    alpha = torch.rand(feature_weights.shape[1], requires_grad=True)
    optim = torch.optim.Adam([alpha], 0.01)
    loss_values = []
    n_sample = data_count_1.sum() + data_count_0.sum()

    for _ in tqdm(range(n_iters * 3), desc="Optimizing rho", leave=False):
        w = cp.Variable(feature_weights.shape[1])
        alpha_fixed = alpha.squeeze().detach().numpy()

        ratio = feature_weights @ w
        q = (data_count_1 + data_count_0) / n_sample
        q = q.reshape(-1, 1)
        q = torch.flatten(q)

        objective = cp.sum_squares(w - alpha_fixed)
        cvxpy_restrictions = restrictions.get_cvxpy_restrictions(
            cvxpy_weights=w,
            feature_weights=feature_weights,
            ratio=ratio,
            q=q,
            n_sample=n_sample,
        )

        prob = cp.Problem(cp.Minimize(objective), cvxpy_restrictions)
        prob.solve()

        if w.value is None:
            print("\nOptimization failed.\n")
            break

        alpha.data = torch.tensor(w.value).float()
        weights_y1 = (feature_weights @ alpha).reshape(*data_count_1.shape)
        weights_y0 = (feature_weights @ alpha).reshape(*data_count_0.shape)
        weighted_counts_1 = weights_y1 * data_count_1
        weighted_counts_0 = weights_y0 * data_count_0

        rho = compute_f_divergence(
            strata_estimands["count"] / dataset_size,
            torch.stack([weighted_counts_0, weighted_counts_1], dim=0) / sample_size,
            type="chi2",
        )

        loss = -rho
        loss_values.append(float(rho.detach().numpy()))

        optim.zero_grad()
        loss.backward()
        optim.step()

    return float(rho.detach().numpy()), loss_values


def run_search(
    restrictions: constraints.Restrictions,
    strata_estimands: Dict[str, torch.Tensor],
    feature_weights: torch.Tensor,
    upper_bound: bool,
    n_iters: int,
    rho: Union[float, None],
) -> Tuple[float, List[float]]:
    """Runs the optimization to find the upper or lower bound for the
    conditional mean.

    Parameters
    ----------
    restrictions : constraints.Restrictions
        Set of constraints to be used in the optimization.
    strata_estimands : Dict[str, torch.Tensor]
        Dictionary with strata counts for each combination of levels.
    feature_weights : torch.Tensor
        Tensor that parametrizes the restrictions matrix
    upper_bound : bool
        Whether to find the upper or lower bound.
    n_iters : int
        Number of iterations to run the optimization for the intervals.
    rho : Union[float, None]
        Value of rho to use in the optimization. This is only relevant
        when calculating bounds with DRO.

    Returns
    -------
    Tuple[float, List[float]]
        Tuple with the optimized bound value and the history of bound values.
    """

    data_count_0 = strata_estimands["count"][0]
    data_count_1 = strata_estimands["count"][1]

    alpha = torch.rand(feature_weights.shape[1], requires_grad=True)
    optim = torch.optim.Adam([alpha], 0.01)
    loss_values = []
    n_sample = data_count_1.sum() + data_count_0.sum()

    for index in tqdm(
        range(n_iters), desc=f"Optimizing Upper Bound: {upper_bound}", leave=False
    ):
        w = cp.Variable(feature_weights.shape[1])
        alpha_fixed = alpha.squeeze().detach().numpy()

        ratio = feature_weights @ w
        q = (data_count_1 + data_count_0) / n_sample
        q = q.reshape(-1, 1)
        q = torch.flatten(q)

        objective = cp.sum_squares(w - alpha_fixed)
        cvxpy_restrictions = restrictions.get_cvxpy_restrictions(
            cvxpy_weights=w,
            feature_weights=feature_weights,
            ratio=ratio,
            q=q,
            n_sample=n_sample,
            rho=rho,
        )

        prob = cp.Problem(cp.Minimize(objective), cvxpy_restrictions)
        prob.solve()

        if w.value is None:
            print("\nOptimization failed.\n")
            break

        alpha.data = torch.tensor(w.value).float()
        weights_y1 = (feature_weights @ alpha).reshape(*data_count_1.shape)
        weights_y0 = (feature_weights @ alpha).reshape(*data_count_0.shape)
        weighted_counts_1 = weights_y1 * data_count_1
        weighted_counts_0 = weights_y0 * data_count_0

        # ===================================#
        w_counts_1 = weighted_counts_1.select(-1, 1)
        w_counts_0 = weighted_counts_0.select(-1, 1)
        # ===================================#

        size = w_counts_1.sum() + w_counts_0.sum()
        conditional_mean = w_counts_1.sum() / size

        loss = -conditional_mean if upper_bound else conditional_mean
        loss_values.append(conditional_mean.detach().numpy())
        optim.zero_grad()
        loss.backward()
        optim.step()

    ret = np.nan
    if len(loss_values) > 0:
        if upper_bound:
            ret = max(loss_values)
        else:
            ret = min(loss_values)

    return ret, loss_values


if __name__ == "__main__":
    os.environ["KMP_DUPLICATE_LIB_OK"] = "True"
    parser = argparse.ArgumentParser(
        description="Run experiments with JSON configuration."
    )
    parser.add_argument("config", help="Path to the JSON configuration file")
    args = parser.parse_args()
    config_dict = read_json(args.config)
    config = parse_config_dict(config_dict)

    plotting_dfs = []
    c_time = datetime.datetime.now()
    timestamp = str(c_time.strftime("%b%d-%H%M"))
    if not os.path.exists(os.path.join("..", "experiment_artifacts", timestamp)):
        os.makedirs(os.path.join("..", "experiment_artifacts", timestamp))

    if not config.dro_restriction_trials:
        dro_restriction_trials = ["count"]
    else:
        dro_restriction_trials = config.dro_restriction_trials

    param_combinations = list(
        itertools.product(
            config.matrix_types,
            config.restriction_trials,
            config.random_seeds,
            dro_restriction_trials,
        )
    )
    for matrix_type, restriction_type, random_seed, dro_restriction_type in tqdm(
        param_combinations, desc="Param Combinations"
    ):
        rng = np.random.default_rng(random_seed)
        if config.dataset_type == "simulation":
            data_loader = SimulationLoader(
                dataset_size=config.dataset_size,
                correlation_coeff=config.correlation_coeff,
                rng=rng,
            )
            dataset = data_loader.load()

            restrictions = constraints.SimulationRestrictions(
                restriction_type=restriction_type,
                n_cov_pairs=config.n_cov_pairs,
                dataset=dataset,
            )
            parametrization = constraints.SimulationParametrization(
                matrix_type=matrix_type,
            )
        elif config.dataset_type == "semi-synthetic":
            data_loader = FolktablesLoader(
                rng=rng, states=config.states, feature_names=config.feature_names
            )
            dataset = data_loader.load()
            restrictions = constraints.SemiSyntheticRestrictions(
                restriction_type=restriction_type,
                n_cov_pairs=config.n_cov_pairs,
                dataset=dataset,
            )
            parametrization = constraints.SemiSyntheticParametrization(
                matrix_type=matrix_type,
            )
        else:
            raise ValueError(f"Invalid dataset type {config.dataset_type}.")

        dataset_size = dataset.population_df_colinear.shape[0]
        sample_size = dataset.sample_df_colinear.shape[0]

        restrictions.build_restriction_values()

        strata_dfs = get_feature_strata(
            df=dataset.sample_df_colinear,
            levels=dataset.levels_colinear,
            target=dataset.target,
        )
        strata_dfs_alternate_outcome = get_feature_strata(
            df=dataset.sample_df_colinear,
            levels=dataset.levels_colinear,
            target=dataset.alternate_outcome,
        )
        strata_dfs_population = get_feature_strata(
            df=dataset.population_df_colinear,
            levels=dataset.levels_colinear,
            target=dataset.target,
        )
        strata_estimands = {
            "count": get_strata_counts(
                strata_dfs=strata_dfs, levels=dataset.levels_colinear
            ),
            "count_plus": get_strata_counts(
                strata_dfs=strata_dfs_alternate_outcome, levels=dataset.levels_colinear
            ),
        }
        if config.n_cov_pairs and restriction_type.startswith("cov"):
            strata_estimands["cov"] = get_strata_covs(
                strata_dfs=strata_dfs,
                levels=dataset.levels_colinear,
                all_feature_means=restrictions.all_feature_means,
            )

        strata_estimands_population = {
            "DRO": get_strata_counts(
                strata_dfs=strata_dfs_population, levels=dataset.levels_colinear
            )
        }

        feature_weights, _ = parametrization.get_feature_weights(
            levels=dataset.levels_colinear,
        )

        restrictions.build_restriction_matrices(
            feature_weights=feature_weights, strata_estimands=strata_estimands
        )

        if restriction_type == "DRO_worst_case":
            restrictions.restriction_type = dro_restriction_type
            rho, rho_history = get_optimized_rho(
                restrictions=restrictions,
                strata_estimands=strata_estimands,
                feature_weights=feature_weights,
                n_iters=config.n_optim_iters,
            )
            restrictions.restriction_type = restriction_type

            rho_perfect = compute_f_divergence(
                strata_estimands_population["DRO"] / dataset_size,
                strata_estimands["count"] / sample_size,
                type="chi2",
            )

            fig, ax = plt.subplots(1, 1, figsize=(10, 5))
            ax.plot(rho_history)
            ax.axhline(
                y=rho_perfect, color="cyan", linestyle="dashed", label="Real Rho"
            )
            ax.set_title("Optimized Rho")
            ax.set_ylabel("Rho")
            ax.set_xlabel("Iteration")
            ax.legend()
            fig.savefig(
                os.path.join(
                    "..",
                    "experiment_artifacts",
                    timestamp,
                    f"rho_{random_seed}_{matrix_type}_{dro_restriction_type}",
                )
            )
        elif restriction_type == "DRO":
            rho = compute_f_divergence(
                strata_estimands_population["DRO"] / dataset_size,
                strata_estimands["count"] / sample_size,
                type="chi2",
            )
        else:
            rho = None

        for trial_idx in tqdm(range(config.n_trials), desc="Trials", leave=False):
            max_bound, max_loss_values = run_search(
                restrictions=restrictions,
                strata_estimands=strata_estimands,
                feature_weights=feature_weights,
                upper_bound=True,
                n_iters=config.n_optim_iters,
                rho=rho,
            )
            min_bound, min_loss_values = run_search(
                restrictions=restrictions,
                strata_estimands=strata_estimands,
                feature_weights=feature_weights,
                upper_bound=False,
                n_iters=config.n_optim_iters,
                rho=rho,
            )

            plotting_dfs.append(
                pd.DataFrame(
                    {
                        "max_bound": max_bound,
                        "min_bound": min_bound,
                        "max_loss": max_loss_values,
                        "min_loss": min_loss_values,
                        "restriction_type": restriction_type,
                        "trial_idx": trial_idx,
                        "matrix_type": matrix_type,
                        "step": np.arange(len(max_loss_values)),
                        "n_cov_pairs": config.n_cov_pairs,
                        "interval_size": max_bound - min_bound,
                        "random_seed": random_seed,
                        "true_conditional_mean": dataset.true_conditional_mean,
                        "empirical_conditional_mean": dataset.empirical_conditional_mean,  # noqa: E501
                        "rho": float(rho) if rho else None,
                        "dro_restriction_type": dro_restriction_type,
                    }
                )
            )

    plotting_df = pd.concat(plotting_dfs).astype(
        {
            "max_loss": np.float64,
            "min_loss": np.float64,
            "step": np.int64,
            "trial_idx": np.int64,
            "max_bound": np.float64,
            "min_bound": np.float64,
            "restriction_type": str,
            "matrix_type": str,
            "n_cov_pairs": np.float64,
            "interval_size": np.float64,
            "random_seed": np.int64,
        }
    )

    fig, ax = plt.subplots(1, 1, figsize=(10, 5))
    ax.axhline(
        y=dataset.true_conditional_mean,
        color="cyan",
        linestyle="dashed",
        label="True Empirical Conditional Mean",
    )
    ax.axhline(
        y=dataset.empirical_conditional_mean,
        color="olive",
        linestyle="dashed",
        label="Empirical Conditional Mean",
    )
    sns.lineplot(
        data=plotting_df,
        x="step",
        y="max_loss",
        hue="restriction_type",
        style="matrix_type",
        palette="tab10",
        ax=ax,
    )
    sns.lineplot(
        data=plotting_df,
        x="step",
        y="min_loss",
        hue="restriction_type",
        style="matrix_type",
        palette="tab10",
        ax=ax,
        legend=False,
    )
    ax.set_title("Conditional Mean")

    fig.savefig(os.path.join("..", "experiment_artifacts", timestamp, "losses"))
    plotting_df.to_csv(
        os.path.join("..", "experiment_artifacts", timestamp, "plotting_df.csv"),
        index=False,
    )

    with open(
        os.path.join("..", "experiment_artifacts", timestamp, "config.json"), "w"
    ) as outp:
        json.dump(config_dict, outp, indent=4)

    print(f"Process finished! Results saved in folder: {timestamp}")
