import argparse
import datetime
import itertools
import json
import os
from typing import Dict, List, Tuple, Union

import constraints
import cvxpy as cp
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
from config import parse_config_dict
from datasets import FolktablesLoader, SimulationLoader
from sklearn.linear_model import LinearRegression, LogisticRegression
from torch.linalg import lstsq
from tqdm import tqdm


def read_json(config_filename):
    with open(config_filename, "r") as config_file:
        config_dict = json.load(config_file)
    return config_dict


def get_feature_strata(
    df: pd.DataFrame, levels: List[List[str]], target: str
) -> Dict[Tuple, pd.DataFrame]:
    """Generate strata for each combination of levels. The strata are
    generated by filtering the dataframe by the combination of levels
    and the target value.

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe with full dataset.
    levels : List[List[str]]
        Levels for each feature we want to create strata for.
    target : str
        Name of the target column.

    Returns
    -------
    Dict[Tuple, pd.DataFrame]
        Dictionary with strata dataframes for each combination of levels.
    """
    levels_combinations = list(itertools.product(*levels))
    strata_dfs = dict()
    for combination in levels_combinations:
        query_string = " & ".join([f"`{level}` == 1" for level in combination])
        strata_dfs[(0,) + combination] = df[df[target] == 0].query(query_string)

    for combination in levels_combinations:
        query_string = " & ".join([f"`{level}` == 1" for level in combination])
        strata_dfs[(1,) + combination] = df[df[target] == 1].query(query_string)
    return strata_dfs


def get_strata_counts(
    strata_dfs: Dict[Tuple, pd.DataFrame], levels: List[List[str]]
) -> torch.Tensor:
    """Get strata counts for each combination of levels. If a strata
    dataframe is empty, we set the count to a small value to avoid
    division by zero.

    Parameters
    ----------
    strata_dfs : Dict[Tuple, pd.DataFrame]
        Dictionary with strata dataframes for each combination of levels.
    levels : List[List[str]]
        Levels for each feature we want to create strata for.

    Returns
    -------
    torch.Tensor
        Tensor with strata counts for each combination of levels.
        The first dimension is the target value (0 or 1) and the
        remaining dimensions are the levels.
    """
    strata_counts = list()
    for strata_df in strata_dfs.values():
        if strata_df.shape[0] == 0:
            strata_counts.append(0.00001)
        else:
            strata_counts.append(strata_df.shape[0])

    output_shape = [2] + [len(level) for level in levels]
    return torch.tensor(strata_counts).reshape(output_shape)


def get_strata_covs(
    strata_dfs: Dict[Tuple, pd.DataFrame],
    levels: List[List[str]],
    all_feature_means: Dict[str, float],
) -> List[torch.Tensor]:
    """Get strata covariances for each combination of levels. If a strata
    dataframe is empty, we set the covariance to zero. We calculate
    the covariance for each pair of feature provided in all_feature_means.

    Parameters
    ----------
    strata_dfs : Dict[Tuple, pd.DataFrame]
        Dictionary with strata dataframes for each combination of levels.
    levels : List[List[str]]
        Levels for each feature we want to create strata for.
    all_feature_means : Dict[str, float]
        Dictionary with two feature names as keys and its means as values.

    Returns
    -------
    List[torch.Tensor]
        List of tensors with strata covariances for each pair of features
        provided in all_feature_means.
    """
    all_strata_covs = []
    for feature_means in all_feature_means:
        feature_names = list(feature_means.keys())
        means = list(feature_means.values())
        strata_covs = list()
        for strata_df in strata_dfs.values():
            strata_cov = np.sum(
                (strata_df[feature_names[0]] - means[0])
                * (strata_df[feature_names[1]] - means[1])
            )
            n_samples = strata_df.shape[0]
            if n_samples > 0:
                strata_cov *= 1 / strata_df.shape[0]
            else:
                strata_cov = 0
            if np.isnan(strata_cov):
                strata_cov = 0
            strata_covs.append(strata_cov)

        output_shape = [2] + [len(level) for level in levels]
        all_strata_covs.append(torch.tensor(strata_covs).reshape(output_shape))
    return all_strata_covs


def compute_f_divergence(
    p: torch.Tensor, q: torch.Tensor, type: str = "chi2"
) -> torch.Tensor:
    """Computes the f-divergence between two distributions.
    We ignore terms where q is zero to avoid division by zero.

    Parameters
    ----------
    p : torch.Tensor
        Terms of the first distribution.
    q : torch.Tensor
        Terms of the second distribution.
    type : str, optional
        Type of divergence to calculate, by default "chi2"

    Returns
    -------
    torch.Tensor
        f-divergence between the two distributions.

    Raises
    ------
    ValueError
        Raises ValueError if the type of divergence is not supported.
    """
    if type == "chi2":
        numerator = (p - q) ** 2
        denominator = q + 1e-8
        # Only include terms where q is non-zero
        mask = denominator > 1e-7
        numerator = numerator[mask]
        denominator = denominator[mask]
        return 0.5 * torch.sum(numerator / denominator)
    else:
        raise ValueError(f"Invalid divergence type {type}.")


def get_optimized_rho(
    restrictions: constraints.Restrictions,
    strata_estimands: Dict[str, torch.Tensor],
    feature_weights: torch.Tensor,
    n_iters: int,
) -> Tuple[float, List[float]]:
    """Gets the rho value that maximizes the f-divergence between the
    population and sample distributions. This is used to calculate a
    realistic benchmark when using DRO.

    Parameters
    ----------
    restrictions : constraints.Restrictions
        Set of constraints to be used in the optimization.
    strata_estimands : Dict[str, torch.Tensor]
        Dictionary with strata counts for each combination of levels.
    feature_weights : torch.Tensor
        Tensor that parametrizes the restrictions matrix
    n_iters : int
        Number of iterations to run the optimization for rho.

    Returns
    -------
    Tuple[float, List[float]]
        Tuple with the optimized rho value and the history of rho values.
    """
    data_count_0 = strata_estimands["count"][0]
    data_count_1 = strata_estimands["count"][1]
    dataset_size = restrictions.dataset.population_df_colinear.shape[0]

    torch.autograd.set_detect_anomaly(True)
    alpha = torch.rand(feature_weights.shape[1], requires_grad=True)
    optim = torch.optim.Adam([alpha], 0.01)
    loss_values = []
    n_sample = data_count_1.sum() + data_count_0.sum()

    for _ in tqdm(range(n_iters * 3), desc="Optimizing rho", leave=False):
        w = cp.Variable(feature_weights.shape[1])
        alpha_fixed = alpha.squeeze().detach().numpy()

        ratio = feature_weights @ w
        q = (data_count_1 + data_count_0) / n_sample
        q = q.reshape(-1, 1)
        q = torch.flatten(q)

        objective = cp.sum_squares(w - alpha_fixed)
        cvxpy_restrictions = restrictions.get_cvxpy_restrictions(
            cvxpy_weights=w,
            feature_weights=feature_weights,
            ratio=ratio,
            q=q,
            n_sample=n_sample,
        )

        prob = cp.Problem(cp.Minimize(objective), cvxpy_restrictions)
        prob.solve()

        if w.value is None:
            print("\nOptimization failed.\n")
            break

        alpha.data = torch.tensor(w.value).float()
        weights_y1 = (feature_weights @ alpha).reshape(*data_count_1.shape)
        weights_y0 = (feature_weights @ alpha).reshape(*data_count_0.shape)
        weighted_counts_1 = weights_y1 * data_count_1
        weighted_counts_0 = weights_y0 * data_count_0

        rho = compute_f_divergence(
            strata_estimands["count"] / dataset_size,
            torch.stack([weighted_counts_0, weighted_counts_1], dim=0) / sample_size,
            type="chi2",
        )

        loss = -rho
        loss_values.append(float(rho.detach().numpy()))

        optim.zero_grad()
        loss.backward()
        optim.step()

    return float(rho.detach().numpy()), loss_values


def create_features_tensor(data: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:
    """Creates a tensor with the features and a tensor with the target
    for the regression models.

    Parameters
    ----------
    data : pd.DataFrame
        Dataframe with full dataset.

    Returns
    -------
    Tuple[torch.Tensor, torch.Tensor]
        Tuple with the features tensor and the target tensor.
    """
    features = data.drop(columns=["Creditability"], inplace=False)
    features = features.to_numpy().astype(np.double)
    features = np.concatenate([features, np.ones((features.shape[0], 1))], axis=-1)
    features_tensor = torch.tensor(features).float()
    target = torch.tensor(data["Creditability"].to_numpy(), dtype=torch.long).float()
    target = torch.unsqueeze(target, -1)
    return features_tensor, target


def assign_weights(
    data: pd.DataFrame,
    hash_map: Dict[Tuple, int],
    weights_features: torch.Tensor,
    matrix_type: str,
) -> torch.Tensor:
    """Assigns weights to each sample in the dataset based on the
    feature combination it belongs to.

    Parameters
    ----------
    data : pd.DataFrame
        Dataframe with full dataset.
    hash_map : Dict[Tuple, int]
        Dictionary with feature combinations as keys and the index
        of the corresponding weight in weights_features as values.
    weights_features : torch.Tensor
        Tensor with the weights for each feature combination.
    matrix_type : str
        Type of matrix to use. Either "unrestricted" or "targeted".

    Returns
    -------
    torch.Tensor
        Tensor with the weights for each sample.

    Raises
    ------
    ValueError
        Raises ValueError if the matrix type is not supported.
    """
    if matrix_type not in ["unrestricted", "targeted"]:
        raise ValueError(f"Invalid matrix type {matrix_type}.")
    weights = []
    data_features = data[data.columns[:-1]]
    for i in range(data_features.shape[0]):
        indexes = data_features.iloc[i, :] == 1
        columns_names = data_features.iloc[i, :][indexes].index
        if matrix_type == "unrestricted":
            tuple_features = (columns_names[-1],) + tuple(
                columns_names[i] for i in range(len(columns_names) - 1)
            )
        elif matrix_type == "targeted":
            tuple_features = (columns_names[0], columns_names[1])
        weight_index = hash_map[tuple_features]
        weight = weights_features[weight_index]
        weights.append(weight)
    return torch.stack(weights)


def run_search(
    statistic: str,
    weights_array: torch.Tensor,
    restrictions: constraints.Restrictions,
    strata_estimands: Dict[str, torch.Tensor],
    feature_weights: torch.Tensor,
    upper_bound: bool,
    n_iters: int,
    rho: Union[float, None],
) -> Tuple[float, List[float]]:
    """Runs the optimization to find the upper or lower bound for a regression
    coefficient.

    Parameters
    ----------
    statistic : str
        Type of statistic to calculate. Either "regression" or "logistic_regression".
    weights_array : torch.Tensor
        Tensor with the weights for each sample.
    restrictions : constraints.Restrictions
        Set of constraints to be used in the optimization.
    strata_estimands : Dict[str, torch.Tensor]
        Dictionary with strata counts for each combination of levels.
    feature_weights : torch.Tensor
        Tensor that parametrizes the restrictions matrix
    upper_bound : bool
        Whether to find the upper or lower bound.
    n_iters : int
        Number of iterations to run the optimization for the intervals.
    rho : Union[float, None]
        Value of rho to use in the optimization. This is only relevant
        when calculating bounds with DRO.

    Returns
    -------
    Tuple[float, List[float]]
        Tuple with the optimized bound value and the history of bound values.
    """

    """Runs the search for the optimal weights."""

    data_count_0 = strata_estimands["count"][0]
    data_count_1 = strata_estimands["count"][1]

    features_tensor = strata_estimands["features_tensor"]
    target = strata_estimands["target"]

    alpha = torch.rand(feature_weights.shape[1], requires_grad=True)
    optim = torch.optim.Adam([alpha], 0.01)
    loss_values = []
    n_sample = data_count_1.sum() + data_count_0.sum()

    for _ in tqdm(
        range(n_iters), desc=f"Optimizing Upper Bound: {upper_bound}", leave=False
    ):
        w = cp.Variable(feature_weights.shape[1])
        alpha_fixed = alpha.squeeze().detach().numpy()

        ratio = feature_weights @ w
        q = (data_count_1 + data_count_0) / n_sample
        q = q.reshape(-1, 1)
        q = torch.flatten(q)

        objective = cp.sum_squares(w - alpha_fixed)
        cvxpy_restrictions = restrictions.get_cvxpy_restrictions(
            cvxpy_weights=w,
            feature_weights=feature_weights,
            ratio=ratio,
            q=q,
            n_sample=n_sample,
            rho=rho,
        )

        prob = cp.Problem(cp.Minimize(objective), cvxpy_restrictions)
        prob.solve()

        if w.value is None:
            print("\nOptimization failed.\n")
            break

        alpha.data = torch.tensor(w.value).float()
        weights = weights_array @ alpha

        if statistic == "regression":
            sq_weights = torch.sqrt(weights)
            W = torch.eye(weights.shape[0]) * sq_weights

            coeff = lstsq(W @ features_tensor, W @ target, driver="gelsd")[0][5]
            objective = coeff[0]

        elif statistic == "logistic_regression":
            features_tensor = features_tensor.double()
            weights = weights.unsqueeze(1)
            # Here we find a local minima for the logistic regression
            # before fitting the theta to have a faster convergence.
            # The algorithm is taken from The elemets of statistical learning pg 121.
            lgit_loss = []
            with torch.no_grad():
                coeff = torch.zeros(features_tensor.shape[1], 1).double()
                for _ in range(10):
                    p = torch.sigmoid(features_tensor @ coeff)
                    Wsqrt = torch.sqrt(weights * p * (1 - p))
                    Winv = 1 / (weights * p * (1 - p))
                    z = features_tensor @ coeff + Winv * 1 / 8 * (target - p)
                    coeff = lstsq(Wsqrt * features_tensor, Wsqrt * z, driver="gels")[0]
                    loglik = (
                        (target * torch.log(p) + (1 - target) * torch.log(1 - p))
                        .mean()
                        .item()
                    )
                    lgit_loss.append(-loglik)

            p = torch.sigmoid(features_tensor @ coeff)
            Wsqrt = torch.sqrt(weights * p * (1 - p))
            Winv = 1 / (weights * p * (1 - p))
            z = features_tensor @ coeff + Winv * (target - p)
            coeff = lstsq(Wsqrt * features_tensor, Wsqrt * z)[0][5]
            objective = coeff[0]
        else:
            raise ValueError(f"Statistic type {statistic} not supported.")

        loss = -objective if upper_bound else objective
        loss_values.append(objective.detach().numpy())

        optim.zero_grad()
        loss.backward()
        optim.step()
    if upper_bound:
        ret = max(loss_values)
    else:
        ret = min(loss_values)

    return ret, loss_values


if __name__ == "__main__":
    os.environ["KMP_DUPLICATE_LIB_OK"] = "True"
    parser = argparse.ArgumentParser(
        description="Run experiments with JSON configuration."
    )
    parser.add_argument("config", help="Path to the JSON configuration file")
    args = parser.parse_args()
    config_dict = read_json(args.config)
    config = parse_config_dict(config_dict)

    plotting_dfs = []
    c_time = datetime.datetime.now()
    timestamp = str(c_time.strftime("%b%d-%H%M"))
    if not os.path.exists(os.path.join("..", "experiment_artifacts", timestamp)):
        os.makedirs(os.path.join("..", "experiment_artifacts", timestamp))

    if not config.dro_restriction_trials:
        dro_restriction_trials = ["count"]
    else:
        dro_restriction_trials = config.dro_restriction_trials

    param_combinations = list(
        itertools.product(
            config.matrix_types,
            config.restriction_trials,
            config.random_seeds,
            dro_restriction_trials,
        )
    )
    for matrix_type, restriction_type, random_seed, dro_restriction_type in tqdm(
        param_combinations, desc="Param Combinations"
    ):
        rng = np.random.default_rng(random_seed)
        if config.dataset_type == "simulation":
            data_loader = SimulationLoader(
                dataset_size=config.dataset_size,
                correlation_coeff=config.correlation_coeff,
                rng=rng,
            )
            dataset = data_loader.load()

            restrictions = constraints.SimulationRestrictions(
                restriction_type=restriction_type,
                n_cov_pairs=config.n_cov_pairs,
                dataset=dataset,
            )
            parametrization = constraints.SimulationParametrization(
                matrix_type=matrix_type,
            )
        elif config.dataset_type == "semi-synthetic":
            data_loader = FolktablesLoader(
                rng=rng,
                states=config.states,
                feature_names=config.feature_names,
                size=config.dataset_size,
                correlation_coeff=config.correlation_coeff,
            )
            dataset = data_loader.load()
            restrictions = constraints.SemiSyntheticRestrictions(
                restriction_type=restriction_type,
                n_cov_pairs=config.n_cov_pairs,
                dataset=dataset,
            )
            parametrization = constraints.SemiSyntheticParametrization(
                matrix_type=matrix_type,
            )
        else:
            raise ValueError(f"Invalid dataset type {config.dataset_type}.")

        dataset_size = dataset.population_df_colinear.shape[0]
        sample_size = dataset.sample_df_colinear.shape[0]

        restrictions.build_restriction_values()

        strata_dfs = get_feature_strata(
            df=dataset.sample_df_colinear,
            levels=dataset.levels_colinear,
            target=dataset.target,
        )
        strata_dfs_alternate_outcome = get_feature_strata(
            df=dataset.sample_df_colinear,
            levels=dataset.levels_colinear,
            target=dataset.alternate_outcome,
        )
        strata_dfs_population = get_feature_strata(
            df=dataset.population_df_colinear,
            levels=dataset.levels_colinear,
            target=dataset.target,
        )
        strata_estimands = {
            "count": get_strata_counts(
                strata_dfs=strata_dfs, levels=dataset.levels_colinear
            ),
            "count_plus": get_strata_counts(
                strata_dfs=strata_dfs_alternate_outcome, levels=dataset.levels_colinear
            ),
        }
        if config.n_cov_pairs and restriction_type.startswith("cov"):
            strata_estimands["cov"] = get_strata_covs(
                strata_dfs=strata_dfs,
                levels=dataset.levels_colinear,
                all_feature_means=restrictions.all_feature_means,
            )

        strata_estimands_population = {
            "DRO": get_strata_counts(
                strata_dfs=strata_dfs_population, levels=dataset.levels_colinear
            )
        }

        feature_weights, hash_map = parametrization.get_feature_weights(
            levels=dataset.levels_colinear
        )

        restrictions.build_restriction_matrices(
            feature_weights=feature_weights, strata_estimands=strata_estimands
        )

        features_tensor, target = create_features_tensor(dataset.sample_df)
        strata_estimands["features_tensor"] = features_tensor
        strata_estimands["target"] = target

        if restriction_type == "DRO_worst_case":
            restrictions.restriction_type = dro_restriction_type
            rho, rho_history = get_optimized_rho(
                restrictions=restrictions,
                strata_estimands=strata_estimands,
                feature_weights=feature_weights,
                n_iters=config.n_optim_iters,
            )
            restrictions.restriction_type = restriction_type

            rho_perfect = compute_f_divergence(
                strata_estimands_population["DRO"] / dataset_size,
                strata_estimands["count"] / sample_size,
                type="chi2",
            )

            fig, ax = plt.subplots(1, 1, figsize=(10, 5))
            ax.plot(rho_history)
            ax.axhline(
                y=rho_perfect, color="cyan", linestyle="dashed", label="Real Rho"
            )
            ax.set_title("Optimized Rho")
            ax.set_ylabel("Rho")
            ax.set_xlabel("Iteration")
            ax.legend()
            fig.savefig(
                os.path.join(
                    "..",
                    "experiment_artifacts",
                    timestamp,
                    f"rho_{random_seed}_{matrix_type}_{dro_restriction_type}",
                )
            )
        elif restriction_type == "DRO":
            rho = compute_f_divergence(
                strata_estimands_population["DRO"] / dataset_size,
                strata_estimands["count"] / sample_size,
                type="chi2",
            )
        else:
            rho = None

        weights_array = assign_weights(
            dataset.sample_df_colinear, hash_map, feature_weights, matrix_type
        )

        X_train_sample = dataset.sample_df.drop("Creditability", axis=1)
        y_train_sample = dataset.sample_df["Creditability"]
        X_train_population = dataset.population_df.drop("Creditability", axis=1)
        y_train_population = dataset.population_df["Creditability"]
        if config.statistic == "regression":
            sample_model = LinearRegression()
            population_model = LinearRegression()

        elif config.statistic == "logistic_regression":
            sample_model = LogisticRegression()
            population_model = LogisticRegression()

        sample_model.fit(X_train_sample, y_train_sample)
        empirical_coef = sample_model.coef_.flatten()[5]

        population_model.fit(X_train_population, y_train_population)
        true_coef = population_model.coef_.flatten()[5]

        for trial_idx in tqdm(range(config.n_trials), desc="Trials", leave=False):
            max_bound, max_loss_values = run_search(
                statistic=config.statistic,
                weights_array=weights_array,
                restrictions=restrictions,
                strata_estimands=strata_estimands,
                feature_weights=feature_weights,
                upper_bound=True,
                n_iters=config.n_optim_iters,
                rho=rho,
            )
            min_bound, min_loss_values = run_search(
                statistic=config.statistic,
                weights_array=weights_array,
                restrictions=restrictions,
                strata_estimands=strata_estimands,
                feature_weights=feature_weights,
                upper_bound=False,
                n_iters=config.n_optim_iters,
                rho=rho,
            )

            plotting_dfs.append(
                pd.DataFrame(
                    {
                        "max_bound": max_bound,
                        "min_bound": min_bound,
                        "max_loss": max_loss_values,
                        "min_loss": min_loss_values,
                        "restriction_type": restriction_type,
                        "trial_idx": trial_idx,
                        "matrix_type": matrix_type,
                        "step": np.arange(len(max_loss_values)),
                        "n_cov_pairs": config.n_cov_pairs,
                        "random_seed": random_seed,
                        "true_coef": true_coef,
                        "empirical_coef": empirical_coef,
                        "rho": float(rho) if rho is not None else None,
                        "dro_restriction_type": dro_restriction_type,
                    }
                )
            )

    plotting_df = pd.concat(plotting_dfs).astype(
        {
            "max_loss": np.float64,
            "min_loss": np.float64,
            "step": np.int64,
            "trial_idx": np.int64,
            "max_bound": np.float64,
            "min_bound": np.float64,
            "restriction_type": str,
            "matrix_type": str,
        }
    )

    fig, ax = plt.subplots(1, 1, figsize=(10, 5))
    ax.axhline(
        y=true_coef,
        color="cyan",
        linestyle="dashed",
        label="True Coefficient",
    )
    ax.axhline(
        y=empirical_coef,
        color="olive",
        linestyle="dashed",
        label="Empirical Coefficient",
    )
    sns.lineplot(
        data=plotting_df,
        x="step",
        y="max_loss",
        hue="restriction_type",
        style="matrix_type",
        palette="tab10",
        ax=ax,
    )
    sns.lineplot(
        data=plotting_df,
        x="step",
        y="min_loss",
        hue="restriction_type",
        style="matrix_type",
        palette="tab10",
        ax=ax,
        legend=False,
    )
    ax.set_title("Estimated Coefficient")
    fig.savefig(os.path.join("..", "experiment_artifacts", timestamp, "losses"))
    plotting_df.to_csv(
        os.path.join("..", "experiment_artifacts", timestamp, "plotting_df.csv"),
        index=False,
    )

    with open(
        os.path.join("..", "experiment_artifacts", timestamp, "config.json"), "w"
    ) as outp:
        json.dump(config_dict, outp, indent=4)

    print(f"Process finished! Results saved in folder: {timestamp}")
